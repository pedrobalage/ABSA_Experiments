{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CRF for Aspect Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "from pycrfsuite import Trainer, Tagger\n",
    "from pycrfsuite import ItemSequence\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.metrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS_TRAIN = '../corpus/SemEvalABSA2016EnglishRestaurants_train.xml'\n",
    "CORPUS_TEST = '../corpus/SemEvalABSA2016EnglishRestaurants_test.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read ReLi sentences with tags\n",
    "dataset = list()\n",
    "tree = etree.parse(CORPUS_TRAIN)\n",
    "\n",
    "tokens_nodes = tree.xpath('.//tokens')\n",
    "for tokens_node in tokens_nodes:\n",
    "    sentence = list()\n",
    "    for word_node in tokens_node:\n",
    "        if word_node.get('opinion')=='true':\n",
    "            tag = 'target'\n",
    "        else:\n",
    "            tag = '_'\n",
    "        sentence.append((word_node.get('form').lower(),\n",
    "                         word_node.get('postag'),\n",
    "                         word_node.get('head'),\n",
    "                         word_node.get('deprel'),\n",
    "                         tag))\n",
    "    if len(sentence) != 0:\n",
    "        dataset.append(sentence)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change head number by the token under that position\n",
    "for sentence in dataset:\n",
    "    for index, item in enumerate(sentence):\n",
    "        word, postag, head, deprel, tag = item\n",
    "        head = int(head)\n",
    "        if head > 0:\n",
    "            head = sentence[int(head) - 1][0]\n",
    "        else:\n",
    "            head = 'ROOT'\n",
    "        sentence[index] = ((word, postag, head, deprel, tag))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('just', 'RB', 'went', 'advmod', '_'),\n",
       "  ('went', 'VBD', 'ROOT', 'root', '_'),\n",
       "  ('here', 'RB', 'went', 'advmod', '_'),\n",
       "  ('for', 'IN', 'bday', 'case', '_'),\n",
       "  ('my', 'PRP$', 'bday', 'nmod:poss', '_'),\n",
       "  ('girlfriends', 'NNS', 'bday', 'compound', '_'),\n",
       "  ('23rd', 'JJ', 'bday', 'amod', '_'),\n",
       "  ('bday', 'NN', 'went', 'nmod', '_'),\n",
       "  ('.', '.', 'went', 'punct', '_')],\n",
       " [('if', 'IN', 'river', 'mark', '_'),\n",
       "  ('you', 'PRP', 'river', 'nsubj', '_'),\n",
       "  (\"'ve\", 'VBP', 'river', 'aux', '_'),\n",
       "  ('ever', 'RB', 'river', 'advmod', '_'),\n",
       "  ('been', 'VBN', 'river', 'cop', '_'),\n",
       "  ('along', 'IN', 'river', 'case', '_'),\n",
       "  ('the', 'DT', 'river', 'det', '_'),\n",
       "  ('river', 'NN', 'have', 'advcl', '_'),\n",
       "  ('in', 'IN', 'weehawken', 'case', '_'),\n",
       "  ('weehawken', 'NNP', 'river', 'nmod', '_'),\n",
       "  ('you', 'PRP', 'have', 'nsubj', '_'),\n",
       "  ('have', 'VBP', 'ROOT', 'root', '_'),\n",
       "  ('an', 'DT', 'idea', 'det', '_'),\n",
       "  ('idea', 'NN', 'have', 'dobj', '_'),\n",
       "  ('of', 'IN', 'top', 'case', '_'),\n",
       "  ('the', 'DT', 'top', 'det', '_'),\n",
       "  ('top', 'NN', 'idea', 'nmod', '_'),\n",
       "  ('of', 'IN', 'view', 'case', '_'),\n",
       "  ('view', 'NN', 'top', 'nmod', 'target'),\n",
       "  ('the', 'DT', 'house', 'det', '_'),\n",
       "  ('chart', 'NN', 'house', 'compound', '_'),\n",
       "  ('house', 'NN', 'has', 'nsubj', '_'),\n",
       "  ('has', 'VBZ', 'view', 'acl:relcl', '_'),\n",
       "  ('to', 'TO', 'offer', 'mark', '_'),\n",
       "  ('offer', 'VB', 'has', 'xcomp', '_'),\n",
       "  ('.', '.', 'have', 'punct', '_')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(dataset):\n",
    "    feature_list = []\n",
    "    for sentence in dataset:\n",
    "        for item in sentence:\n",
    "            word, postag, head, deprel, tag = item\n",
    "            features = {}\n",
    "            features['bias'] = 1\n",
    "            features['word'] = word\n",
    "            features['postag'] = postag\n",
    "            features['head'] = head\n",
    "            feature_list.append(features)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_labels(dataset):\n",
    "    labels = []\n",
    "    for sentence in dataset:\n",
    "        for item in sentence:\n",
    "            word, postag, head, deprel, tag = item\n",
    "            labels.append(tag)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crf_train(dataset):\n",
    "    trainer = Trainer(verbose=False)\n",
    "\n",
    "    # default parameters\n",
    "    trainer.select(algorithm='lbfgs', type='crf1d')\n",
    "\n",
    "    # set from examples\n",
    "    trainer.set_params({\n",
    "        'c1': 0.1,   # coefficient for L1 penalty\n",
    "        'c2': 1.0,  # coefficient for L2 penalty\n",
    "    })\n",
    "\n",
    "    X_train = ItemSequence(extract_features(dataset))\n",
    "    Y_train = extract_labels(dataset)\n",
    "\n",
    "    trainer.append(X_train, Y_train)\n",
    "    trainer.train('model.crfsuite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reports(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(y_true)\n",
    "    y_pred_combined = lb.transform(y_pred)\n",
    "\n",
    "    tagset = set(lb.classes_)\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "\n",
    "    accuracy = accuracy_score(y_true_combined, y_pred_combined) * 100\n",
    "    print('Accuracy: {:.2f}%\\n'.format(accuracy))\n",
    "\n",
    "    print('Classification Report:\\n')\n",
    "    print(classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels=[class_indices[cls] for cls in tagset],\n",
    "        target_names=tagset))\n",
    "\n",
    "    cm = ConfusionMatrix(y_true, y_pred)\n",
    "    print (cm.pretty_format(sort_by_count=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tagger_reports(tagger):\n",
    "    info = tagger.info()\n",
    "\n",
    "    def print_transitions(trans_features):\n",
    "        for (label_from, label_to), weight in trans_features:\n",
    "            print(\"%-8s -> %-8s %0.4f\" % (label_from, label_to, weight))\n",
    "\n",
    "    print(\"\\nTop likely transitions:\")\n",
    "    print_transitions(Counter(info.transitions).most_common(15))\n",
    "    print(\"\\nTop unlikely transitions:\")\n",
    "    print_transitions(Counter(info.transitions).most_common()[-15:])\n",
    "\n",
    "    def print_state_features(state_features):\n",
    "        for (attr, label), weight in state_features:\n",
    "            print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "    print(\"\\nTop positive:\")\n",
    "    print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "    print(\"\\nTop negative:\")\n",
    "    print_state_features(Counter(info.state_features).most_common()[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(sentence, index):\n",
    "    if index == 0:\n",
    "        left_word = ''\n",
    "    else:\n",
    "        left_word = sentence[index - 1][0]\n",
    "    if index == len(sentence) - 1:\n",
    "        right_word = ''\n",
    "    else:\n",
    "        right_word = sentence[index + 1][0]\n",
    "    context_word = left_word + '_' + sentence[index][0] + '_' + right_word\n",
    "    return context_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def statistcs(dataset, y_pred, lookup_tag, threshold):\n",
    "    count = 0\n",
    "    freq = dict()\n",
    "    freq_tag = dict()\n",
    "    correct = dict()\n",
    "    incorrect = dict()\n",
    "    misclassified = dict()\n",
    "    context = dict()\n",
    "\n",
    "    for sentence in dataset:\n",
    "        for index, item in enumerate(sentence):\n",
    "            word, postag, head, deprel, tag = item\n",
    "            freq[word] = freq.get(word, 0) + 1\n",
    "            if tag == lookup_tag:\n",
    "                freq_tag[word] = freq_tag.get(word, 0) + 1\n",
    "                if word not in context:\n",
    "                    context[word] = dict()\n",
    "                context_word = get_context(sentence, index)\n",
    "                context[word][context_word] = context[word].get(context_word, 0) + 1\n",
    "\n",
    "                if tag == y_pred[count]:\n",
    "                    correct[word] = correct.get(word, 0) + 1\n",
    "                else:\n",
    "                    incorrect[word] = incorrect.get(word, 0) + 1\n",
    "            elif y_pred[count] == lookup_tag:\n",
    "                misclassified[word] = misclassified.get(word, 0) + 1\n",
    "            count += 1\n",
    "\n",
    "    print ('tag\\tword\\t\\tfreq\\tfreq_tag\\tcorrect\\tincorrect\\tmisclassified')\n",
    "\n",
    "    for word, _ in sorted(freq_tag.items(), key=itemgetter(1), reverse=True)[:threshold]:\n",
    "        print ('{}{}{}{}{}{}{}'.format(lookup_tag + '   ',\n",
    "                                       word + ' ' * (25 - len(word)),\n",
    "                                       str(freq.get(word, 0)) + '\\t',\n",
    "                                       str(freq_tag.get(word, 0)) + '\\t\\t',\n",
    "                                       str(correct.get(word, 0)) + '\\t\\t',\n",
    "                                       str(incorrect.get(word, 0)) + '\\t\\t',\n",
    "                                       str(misclassified.get(word, 0))))\n",
    "    # ', '.join([c + '(' + str(f) + ')' for c, f in sorted(context[word].items(), key=itemgetter(1), reverse=True)][:6])\n",
    "\n",
    "    # statistcs for each postag\n",
    "    count = 0\n",
    "    freq = dict()\n",
    "    freq_tag = dict()\n",
    "    correct = dict()\n",
    "    incorrect = dict()\n",
    "    misclassified = dict()\n",
    "    context = dict()\n",
    "    for sentence in dataset:\n",
    "        for index, item in enumerate(sentence):\n",
    "            word, postag, head, deprel, tag = item\n",
    "            freq[postag] = freq.get(postag, 0) + 1\n",
    "            if tag == lookup_tag:\n",
    "                freq_tag[postag] = freq_tag.get(postag, 0) + 1\n",
    "                if tag == y_pred[count]:\n",
    "                    correct[postag] = correct.get(postag, 0) + 1\n",
    "                else:\n",
    "                    incorrect[postag] = incorrect.get(postag, 0) + 1\n",
    "            elif y_pred[count] == lookup_tag:\n",
    "                misclassified[postag] = misclassified.get(postag, 0) + 1\n",
    "            count += 1\n",
    "\n",
    "    print ('tag\\tpos\\t\\tfreq\\tfreq_tag\\tcorrect\\tincorrect\\tmisclassified')\n",
    "\n",
    "    for postag, _ in sorted(freq.items(), key=itemgetter(1), reverse=True):\n",
    "        print ('{}{}{}{}{}{}{}'.format(lookup_tag + '\\t',\n",
    "                                       postag.upper() + ' ' * (25 - len(postag)),\n",
    "                                       str(freq.get(postag, 0)) + '\\t\\t',\n",
    "                                       str(freq_tag.get(postag, 0)) + '\\t\\t',\n",
    "                                       str(correct.get(postag, 0)) + '\\t\\t',\n",
    "                                       str(incorrect.get(postag, 0)) + '\\t\\t',\n",
    "                                       str(misclassified.get(postag, 0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crf_train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read ReLi sentences with tags\n",
    "dataset = list()\n",
    "tree = etree.parse(CORPUS_TEST)\n",
    "\n",
    "tokens_nodes = tree.xpath('.//tokens')\n",
    "for tokens_node in tokens_nodes:\n",
    "    sentence = list()\n",
    "    for word_node in tokens_node:\n",
    "        if word_node.get('opinion')=='true':\n",
    "            tag = 'target'\n",
    "        else:\n",
    "            tag = '_'\n",
    "        sentence.append((word_node.get('form').lower(),\n",
    "                         word_node.get('postag'),\n",
    "                         word_node.get('head'),\n",
    "                         word_node.get('deprel'),\n",
    "                         tag))\n",
    "    if len(sentence) != 0:\n",
    "        dataset.append(sentence)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change head number by the token under that position\n",
    "for sentence in dataset:\n",
    "    for index, item in enumerate(sentence):\n",
    "        word, postag, head, deprel, tag = item\n",
    "        head = int(head)\n",
    "        if head > 0:\n",
    "            head = sentence[int(head) - 1][0]\n",
    "        else:\n",
    "            head = 'ROOT'\n",
    "        sentence[index] = ((word, postag, head, deprel, tag))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('yum', 'NN', 'ROOT', 'root', '_'), ('!', '.', 'yum', 'punct', '_')],\n",
       " [('serves', 'VBZ', 'ROOT', 'root', '_'),\n",
       "  ('really', 'RB', 'sushi', 'advmod', '_'),\n",
       "  ('good', 'JJ', 'sushi', 'amod', '_'),\n",
       "  ('sushi', 'NN', 'serves', 'dobj', 'target'),\n",
       "  ('.', '.', 'serves', 'punct', '_')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = dataset\n",
    "testset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.08%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          _       0.95      0.98      0.96      9015\n",
      "     target       0.69      0.47      0.56       928\n",
      "\n",
      "avg / total       0.92      0.93      0.92      9943\n",
      "\n",
      "       |         t |\n",
      "       |         a |\n",
      "       |         r |\n",
      "       |         g |\n",
      "       |         e |\n",
      "       |    _    t |\n",
      "-------+-----------+\n",
      "     _ |<8820> 195 |\n",
      "target |  493 <435>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = Tagger()\n",
    "tagger.open('model.crfsuite')\n",
    "\n",
    "X_test = ItemSequence(extract_features(testset))\n",
    "y_pred = tagger.tag(X_test)\n",
    "y_true = extract_labels(testset)\n",
    "\n",
    "reports(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top likely transitions:\n",
      "_        -> _        1.2930\n",
      "target   -> target   -0.2272\n",
      "target   -> _        -1.1158\n",
      "_        -> target   -1.1179\n",
      "\n",
      "Top unlikely transitions:\n",
      "_        -> _        1.2930\n",
      "target   -> target   -0.2272\n",
      "target   -> _        -1.1158\n",
      "_        -> target   -1.1179\n",
      "\n",
      "Top positive:\n",
      "2.944413 target word:decor\n",
      "2.255491 target word:ambience\n",
      "2.168018 target word:service\n",
      "2.130844 target postag:NNS\n",
      "1.963599 target word:staff\n",
      "1.887555 _      word:great\n",
      "1.873808 target postag:NN\n",
      "1.821644 _      postag:.\n",
      "1.807634 target word:view\n",
      "1.770234 target postag:NNP\n",
      "1.763494 _      word:is\n",
      "1.674473 target word:portions\n",
      "1.629019 target word:atmosphere\n",
      "1.587426 target word:food\n",
      "1.569808 target head:skiline\n",
      "1.518295 target word:waiter\n",
      "1.439020 _      head:restaurants\n",
      "1.414374 target postag:NNPS\n",
      "1.367203 _      word:restaurants\n",
      "1.359148 _      word:quality\n",
      "\n",
      "Top negative:\n",
      "-0.952573 _      word:dessert\n",
      "-0.968065 _      word:meal\n",
      "-0.977329 _      word:japanese\n",
      "-1.002008 _      word:indian\n",
      "-1.015693 _      word:manager\n",
      "-1.033268 _      word:dishes\n",
      "-1.054971 _      head:seasons\n",
      "-1.107058 _      word:place\n",
      "-1.145909 _      head:jekyll\n",
      "-1.232589 _      word:pizza\n",
      "-1.282003 _      word:waitress\n",
      "-1.414374 _      postag:NNPS\n",
      "-1.518295 _      word:waiter\n",
      "-1.587426 _      word:food\n",
      "-1.629019 _      word:atmosphere\n",
      "-1.770234 _      postag:NNP\n",
      "-1.873808 _      postag:NN\n",
      "-1.963599 _      word:staff\n",
      "-2.130844 _      postag:NNS\n",
      "-2.168018 _      word:service\n"
     ]
    }
   ],
   "source": [
    "tagger_reports(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
