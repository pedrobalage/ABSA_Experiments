{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to process SemEval 2016 Task 5 - Aspect Based Sentiment Analysis (ABSA) corpus using Stanford CoreNLP dependency parser\n",
    "\n",
    "Task/Slot being evaluated:\n",
    "\n",
    "    SemEval 2016 Task 5 - Aspect Based Sentiment Analysis (ABSA) (http://alt.qcri.org/semeval2016/task5/)\n",
    "    Subtask 1: Sentence-Level ABSA\n",
    "    Slot 2: Opinion Target Expression (OTE) - (Restaurants domain)\n",
    "\n",
    "Corpus reference:\n",
    " \n",
    "    Pontiki, M., Galanis, D., Papageorgiou, H., Manandhar, S., & Androutsopoulos, I. To Appear. \n",
    "    Semeval-2016 task 05: Aspect based sentiment analysis. \n",
    "    In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016),\n",
    "    Association for Computational Linguistics.\n",
    "    http://alt.qcri.org/semeval2016/task5/index.php?id=data-and-tools\n",
    "\n",
    "\n",
    "Stanford CoreNLP:\n",
    "\n",
    "    Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J. R., Bethard, S., & McClosky, D. (2014, June).\n",
    "    The Stanford CoreNLP Natural Language Processing Toolkit. In ACL (System Demonstrations) (pp. 55-60).\n",
    "    http://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf\n",
    "    http://stanfordnlp.github.io/CoreNLP/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startanford core nlp server\n",
    "\n",
    "* Download Stanford CoreNLP - http://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip\n",
    "* Move ziped folder to /opt/stanford-corenlp\n",
    "* Start the corenlp server\n",
    "\n",
    "```bash\n",
    "$ java -mx6g -cp \"/opt/stanford-corenlp/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets test the Stanford CoreNLP server\n",
    "import json\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    # python 3\n",
    "    from urllib.parse import urlencode\n",
    "except:\n",
    "    # python 2\n",
    "    from urllib import urlencode\n",
    "    \n",
    "properties = {'ssplit.isOneSentence':'true',\n",
    "              'outputFormat': 'conll'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use json dumps function to convert the nested dictionary to a string\n",
    "properties_val = json.dumps(properties)\n",
    "params = {'properties': properties_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we encode the URL params using urllib because we need a URL with GET parameters\n",
    "# even though we are making a POST request. The POST data is the text itself.\n",
    "encoded_params = urlencode(params)\n",
    "url = '{url}:{port}/?{params}'.format(url='http://localhost',\n",
    "                                      port='9000', \n",
    "                                      params=encoded_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tSaul\tSaul\tNNP\tPERSON\t5\tnsubj\n",
      "2\tis\tbe\tVBZ\tO\t5\tcop\n",
      "3\tthe\tthe\tDT\tO\t5\tdet\n",
      "4\tbest\tbest\tJJS\tO\t5\tamod\n",
      "5\trestaurant\trestaurant\tNN\tO\t0\tROOT\n",
      "6\ton\ton\tIN\tO\t8\tcase\n",
      "7\tSmith\tSmith\tNNP\tORGANIZATION\t8\tcompound\n",
      "8\tStreet\tStreet\tNNP\tORGANIZATION\t5\tnmod\n",
      "9\tand\tand\tCC\tO\t8\tcc\n",
      "10\tin\tin\tIN\tO\t11\tcase\n",
      "11\tBrooklyn\tBrooklyn\tNNP\tLOCATION\t8\tconj\n",
      "12\t.\t.\t.\tO\t5\tpunct\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'Saul is the best restaurant on Smith Street and in Brooklyn.'\n",
    "headers = {'Content-Type': 'text/plain;charset=utf-8'}\n",
    "response = requests.post(url, text.encode('utf-8'), headers=headers)\n",
    "print (response.content.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CORPUS_FILES = [('../corpus/SemEval_ABSA2016/SemEvalABSA2016EnglishRestaurants_trial+train.xml',\n",
    "                 '../corpus/SemEvalABSA2016EnglishRestaurants_train.xml'),\n",
    "                ('../corpus/SemEval_ABSA2016/SemEvalABSA2016EnglishRestaurants_test.xml',\n",
    "                 '../corpus/SemEvalABSA2016EnglishRestaurants_test.xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = etree.XMLParser(remove_blank_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: \"../corpus/SemEval_ABSA2016/SemEvalABSA2016EnglishRestaurants_trial+train.xml\"\n",
      "Processing file: \"../corpus/SemEval_ABSA2016/SemEvalABSA2016EnglishRestaurants_test.xml\"\n"
     ]
    }
   ],
   "source": [
    "for input_filename, output_filename in CORPUS_FILES:    \n",
    "    xmldoc = etree.parse(input_filename, parser)\n",
    "    print('Processing file: \"{}\"'.format(input_filename))\n",
    "    for sentence_node in xmldoc.xpath('.//sentence'):\n",
    "                    \n",
    "        sentence_string = sentence_node.xpath('string(./text/text())')\n",
    "                \n",
    "        # process the sentence in Stanford CoreNLP\n",
    "        properties = {'ssplit.isOneSentence':'true','outputFormat': 'xml'}\n",
    "        properties_val = json.dumps(properties)\n",
    "        params = {'properties': properties_val}\n",
    "        encoded_params = urlencode(params)\n",
    "        url = '{url}:{port}/?{params}'.format(url='http://localhost',\n",
    "                                              port='9000', \n",
    "                                              params=encoded_params)\n",
    "        response = requests.post(url, sentence_string.encode('utf-8'), headers=headers)\n",
    "\n",
    "        # parse xml removing the header\n",
    "        output= response.content.decode('utf8')\n",
    "        parsed_sentence = etree.fromstring(output[101:], parser)\n",
    "        \n",
    "        # compile list of opinions\n",
    "        opinions = list()\n",
    "        for opinion in sentence_node.xpath('.//Opinion'):\n",
    "            opinions.append(dict(opinion.items()))\n",
    "        \n",
    "        # compile a dict of dependencies\n",
    "        dependencies = dict()\n",
    "        for dep in parsed_sentence.xpath('.//dependencies[@type=\"basic-dependencies\"]/dep'):\n",
    "            idx= dep.xpath('string(dependent/@idx)')\n",
    "            dep_type = dep.get('type')\n",
    "            head = dep.xpath('string(governor/@idx)')\n",
    "            dependencies[idx] = (head, dep_type)\n",
    "                    \n",
    "        # build the tokens node\n",
    "        tokens_node = etree.SubElement(sentence_node, 'tokens')\n",
    "        \n",
    "        \n",
    "        for token_node in parsed_sentence.xpath('.//token'):\n",
    "            word_node = etree.SubElement(tokens_node, 'word')\n",
    "                    \n",
    "            word_node.set('id', token_node.get('id'))\n",
    "            word_node.set('form', token_node.xpath('string(./word)'))\n",
    "            word_node.set('from', token_node.xpath('string(./CharacterOffsetBegin)'))\n",
    "            word_node.set('to', token_node.xpath('string(./CharacterOffsetEnd)'))\n",
    "            word_node.set('base', token_node.xpath('string(./lemma)'))\n",
    "            word_node.set('postag', token_node.xpath('string(./POS)'))\n",
    "            word_node.set('ner', token_node.xpath('string(./NER)'))\n",
    "            word_node.set('opinion', 'false')\n",
    "            \n",
    "            if token_node.get('id') in dependencies:\n",
    "                word_node.set('head', dependencies[token_node.get('id')][0])\n",
    "                word_node.set('deprel', dependencies[token_node.get('id')][1])\n",
    "            else:\n",
    "                word_node.set('head', '_')\n",
    "                word_node.set('deprel', '_')\n",
    "                                    \n",
    "            \n",
    "            # check if token is part of a opinion\n",
    "            for opinion in opinions:                                 \n",
    "                if (int(opinion.get('from'))  <= int(word_node.get('from')) and \n",
    "                    int(opinion.get('to'))    >= int(word_node.get('to'))):\n",
    "                    word_node.set('opinion', 'true')                    \n",
    "                    word_node.set('polarity', opinion.get('polarity'))                \n",
    "                      \n",
    "        # get and append the tokens node into corpus xml        \n",
    "        sentence_node.append(tokens_node)\n",
    "                    \n",
    "    # save the corpus with new annotation\n",
    "    etree.ElementTree(xmldoc.getroot()).write(output_filename, encoding='utf8', xml_declaration=True, pretty_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
